sanctity.vice= -mean(sanctity.vice, na.rm=TRUE)
) %>%
select(date2, care.virtue,care.vice,
fairness.virtue,fairness.vice,
loyalty.virtue,loyalty.vice,
authority.virtue,authority.vice,
sanctity.virtue,sanctity.vice
) %>%
mutate(care.virtue=roll_mean(as.numeric(care.virtue),j,na.rm = FALSE,fill=NA),
care.vice=roll_mean(as.numeric(care.vice),j,na.rm = TRUE,fill=NA),
fairness.virtue=roll_mean(as.numeric(fairness.virtue),j,na.rm = TRUE,fill=NA),
fairness.vice=roll_mean(as.numeric(fairness.vice),j,na.rm = TRUE,fill=NA),
loyalty.virtue=roll_mean(as.numeric(loyalty.virtue),j,na.rm = TRUE,fill=NA),
loyalty.vice=roll_mean(as.numeric(loyalty.vice),j,na.rm = TRUE,fill=NA),
authority.virtue=roll_mean(as.numeric(authority.virtue),j,na.rm = TRUE,fill=NA),
authority.vice=roll_mean(as.numeric(authority.vice),j,na.rm = TRUE,fill=NA),
sanctity.virtue=roll_mean(as.numeric(sanctity.virtue),j,na.rm = TRUE,fill=NA),
sanctity.vice=roll_mean(as.numeric(sanctity.vice),j,na.rm = TRUE,fill=NA)
)
df$date2<-as.factor(df$date2)
df_emo<-melt(df_emo,id="date2")
col=c("orange3","orange1","chartreuse3","chartreuse1","skyblue3","skyblue2","grey50","grey90","purple3", "purple1")
g10<-ggplot(data = df_emo, aes(x = date2, y = value, group = variable)) +
geom_line(aes(color=variable), size =0.8)+
theme_minimal()+
# geom_smooth(method = "gam",aes(color=variable))+
labs(title ="Evolution des émotions", x=NULL, subtitle = "lissage: 60 jours",y="valeur")
+
geom_vline(xintercept = as.POSIXct("2016-11-04",format="%Y-%m-%d"), linetype="solid",color = "grey40", alpha=.5,size=3)
g10<-ggplot(data = df_emo, aes(x = date2, y = value, group = variable)) +
geom_line(aes(color=variable), size =0.8)+
theme_minimal()+
# geom_smooth(method = "gam",aes(color=variable))+
labs(title ="Evolution des émotions", x=NULL, subtitle = "lissage: 60 jours",y="valeur")+
geom_vline(xintercept = as.POSIXct("2016-11-04",format="%Y-%m-%d"), linetype="solid",color = "grey40", alpha=.5,size=3)
g10
+scale_color_manual(values=col)
g10+scale_color_manual(values=col)
library(quanteda)
toks<-tokens(df$text)
col <-toks %>%
tokens_remove(stopwords("en")) %>%
textstat_collocations(min_count = 10,size=2:4, tolower = FALSE)%>% filter(abs(z)>5)
head(col, 20)
toks_comp <- tokens_compound(toks, pattern = col)
library(quanteda)
toks<-tokens(df$text)
col <-toks %>%
tokens_remove(stopwords("en")) %>%
tokens_select(pattern = "^[A-Z]", valuetype = "regex",
case_insensitive = FALSE, padding = TRUE) %>%
textstat_collocations(min_count = 10,size=2:4, tolower = FALSE)%>% filter(abs(z)>5)
head(col, 40)
toks_comp <- tokens_compound(toks_comp, pattern = col)
dfmat_tweets <- toks_comp %>%
dfm(remove_punct = TRUE, remove_url = TRUE, remove_symbols = TRUE) %>%
dfm_remove(pattern = c("*.tt", "*.uk", "*.com", "rt", "#*", "@*")) %>%
dfm_remove(pattern = stopwords("en"))
#ndoc(dfmat_tweets)
#topfeatures(dfmat_tweets)
dfmat_tweets %>%
textstat_frequency(n = 80) %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point(color="firebrick") +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal()
textplot_wordcloud(dfmat_tweets,min_count=200)
textplot_wordcloud(dfmat_tweets,min_count=300)
#l annee de la campgne
df$Year<-as.numeric(format(df$date, "%Y")) # annnée
foo<-df %>%select(-date2) %>% filter(Year>2019)
toks<-tokens(foo$text)
col <-toks %>%
tokens_remove(stopwords("en")) %>%
textstat_collocations(min_count = 10,size=2:4, tolower = FALSE)%>% filter(abs(z)>5)
head(col, 20)
toks_comp <- tokens_compound(toks, pattern = col)
dfmat_tweets <- toks_comp %>%
dfm(remove_punct = TRUE, remove_url = TRUE, remove_symbols = TRUE) %>%
dfm_remove(pattern = c("*.tt", "*.uk", "*.com", "rt", "#*", "@*","amp")) %>%
dfm_remove(pattern = stopwords("en"))
textplot_wordcloud(dfmat_tweets,min_count=100, color = rev(RColorBrewer::brewer.pal(10, "RdBu")))
textplot_wordcloud(dfmat_tweets,min_count=300, color = rev(RColorBrewer::brewer.pal(10, "RdBu")))
textplot_wordcloud(dfmat_tweets,min_count=30, color = rev(RColorBrewer::brewer.pal(10, "RdBu")))
textplot_wordcloud(dfmat_tweets,min_count=50, color = rev(RColorBrewer::brewer.pal(10, "RdBu")))
tag_fcm <- fcm(dfmat_tweets)
toptag <- names(topfeatures(tag_fcm, 500))
head(tag_fcm)
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag)
textplot_network(topgat_fcm, min_freq = 30,color="pink", edge_alpha = 0.2, edge_size = 2,vertex_size=.7, vertex_labelsize = 3.5)
library("quanteda.textmodels")
#mylsa <- textmodel_lsa(dfmat_tweets)
#si le fichier on lit nos données brutes, et on nettoie le corpus des stop words
cnlp_init_udpipe(model_name  = "french")
#lecture de l'ensemble de nos tweets
obj<-df$text
#Annotation des tweets afin de pouvoir identifier les stopwords
Vocab<-cnlp_annotate(obj)
#Annotation des tweets afin de pouvoir identifier les stopwords
Vocab<-cnlp_annotate(obj,verbose=10000)
#filtrage sur les stopwords
foo<-as.data.frame(Vocab[c("token")])
ggplot(foo,aes(x=token.relation))+
geom_bar()+
coord_flip()
#on filtte advebes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.upos %in% c('ADV','ADJ','VERB', 'NOUN'))
#on crée une chaine de caractère qui concatène les lemmes filtrés
all_tweets <- paste(updated_vocab['token.lemma'], sep= " ")
#on génère le fichier de ces twitts " purifiés"
write.table(all_tweets, file="tweets.txt")
#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
prep_word2vec(origin="tweets.txt",destination="confinement_vec.txt",lowercase=T,bundle_ngrams=4, Verbose=FALSE)
#library(cleanNLP) pour les POS et Dépendences syntaxiques
#library(wordvectors)
# initialisation du modèle , ici udpipe, mais aussi spacy corenlp ou stringi
#(un travail devrait être de comprer ces méthodes par le taux de couvrement!!!!)
cnlp_init_udpipe(model_name  = "french")
#library(cleanNLP) pour les POS et Dépendences syntaxiques
library(wordvectors)
remotes::install_github("bmschmidt/wordVectors")
#library(cleanNLP) pour les POS et Dépendences syntaxiques
install.packages("remotes")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
df<-tweets_11_06_2020 <- read_csv("~/AtelierR/Trump/tweets_11-06-2020.csv")
# Attention ajuster à sa configuration de dossier
df$date2<-as.POSIXlt(df$date)
remotes::install_github("bmschmidt/wordVectors")
#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
prep_word2vec(origin="tweets.txt",destination="confinement_vec.txt",lowercase=T,bundle_ngrams=4, Verbose=FALSE)
#library(cleanNLP) pour les POS et Dépendences syntaxiques
install.packages("remotes")
install.packages("remotes")
remotes::install_github("bmschmidt/wordVectors")
#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
prep_word2vec(origin="tweets.txt",destination="confinement_vec.txt",lowercase=T,bundle_ngrams=4, Verbose=FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
df<-tweets_11_06_2020 <- read_csv("~/AtelierR/Trump/tweets_11-06-2020.csv")
# Attention ajuster à sa configuration de dossier
df$date2<-as.POSIXlt(df$date)
model = train_word2vec("trump_vec.txt","trump.bin",vectors=250
,threads=4,window=5,iter=10,negative_samples=0,force=TRUE, min_count=50)
library(wordVectors)
model = train_word2vec("trump_vec.txt","trump.bin",vectors=250
,threads=4,window=5,iter=10,negative_samples=0,force=TRUE, min_count=50)
foo<-model %>% closest_to(~"mask",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"Biden",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"biden",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
View(foo)
View(foo)
foo<-model %>% closest_to(~"biden",10)
foo = foo [-1:-3,]
foo<-model %>% closest_to(~"trump",10)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"great",10)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"President",10)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
View(Vocab)
View(Vocab)
#on filtte adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.lemma %in% c('ADV','ADJ','VERB', 'NOUN'))
#filtrage sur les stopwords
foo<-as.data.frame(Vocab[c("token")])
#on filtte adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.lemma %in% c('ADV','ADJ','VERB', 'NOUN'))
#on crée une chaine de caractère qui concatène les lemmes filtrés
all_tweets <- paste(updated_vocab['token.lemma'], sep= " ")
# initialisation du modèle , ici udpipe, mais aussi spacy corenlp ou stringi
#(un travail devrait être de comprer ces méthodes par le taux de couvrement!!!!)
cnlp_init_udpipe(model_name  = "english")
library(cleanNLP) pour les POS et Dépendences syntaxiques
library(cleanNLP) #pour les POS et Dépendences syntaxiques
# initialisation du modèle , ici udpipe, mais aussi spacy corenlp ou stringi
#(un travail devrait être de comprer ces méthodes par le taux de couvrement!!!!)
cnlp_init_udpipe(model_name  = "english")
#lecture de l'ensemble de nos tweets
obj<-df$text
#Annotation des tweets afin de pouvoir identifier les stopwords
t0<-Sys.time() #date de départ
Vocab<-cnlp_annotate(obj,verbose=5000)
t1<-Sys.time() #date de fin.... juste pour controler une opération qui peut prendre 40 mn sur un processeeur 4 couer à 3.6ghz et 32g de ram.
#filtrage sur les stopwords
foo<-as.data.frame(Vocab[c("token")])
ggplot(foo,aes(x=token.upos))+
geom_bar()+coord_flip() +
theme_minimal()
ggplot(foo,aes(x=token.relation))+
geom_bar()+
coord_flip()
#on filtte adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.lemma %in% c('ADV','ADJ','VERB', 'NOUN'))
#on crée une chaine de caractère qui concatène les lemmes filtrés
all_tweets <- paste(updated_vocab['token.lemma'], sep= " ")
#on génère le fichier de ces twitts " purifiés"
write.table(all_tweets, file="tweets.txt")
#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
prep_word2vec(origin="tweets.txt",destination="trump_vec.txt",lowercase=T,bundle_ngrams=4)
model = train_word2vec("trump_vec.txt","trump.bin",vectors=250
,threads=4,window=5,iter=10,negative_samples=0,force=TRUE, min_count=50)
#filtrage sur les stopwords
foo<-as.data.frame(Vocab[c("token")])
#on filtte adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.lemma %in% c('ADV','ADJ','VERB', 'NOUN'))
#on filtte adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.token %in% c('ADV','ADJ','VERB', 'NOUN'))
#filtrage sur les stopwords
foo<-as.data.frame(Vocab[c("token")])
ggplot(foo,aes(x=token.upos))+
geom_bar()+coord_flip() +
theme_minimal()
ggplot(foo,aes(x=token.relation))+
geom_bar()+
coord_flip()
#on filttre adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.upos%in% c('ADV','ADJ','VERB', 'NOUN'))
#on crée une chaine de caractère qui concatène les lemmes filtrés
all_tweets <- paste(updated_vocab['token.lemma'], sep= " ")
#on génère le fichier de ces twitts " purifiés"
write.table(all_tweets, file="tweets.txt")
#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
prep_word2vec(origin="tweets.txt",destination="trump_vec.txt",lowercase=T,bundle_ngrams=4)
model = train_word2vec("trump_vec.txt","trump.bin",vectors=250
,threads=4,window=5,iter=10,negative_samples=0,force=TRUE, min_count=50)
foo<-model %>% closest_to(~"President",10)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"president",10)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"biden",10)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"joebiden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~ "mask",60)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la solidarite")
g1
foo<-model %>% closest_to(~ "election",60)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la solidarite")
g1
foo<-model %>% closest_to(~ "election",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la solidarite")
g1
foo<-model %>% closest_to(~"election"+"biden",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la protection")
g1
foo<-model %>% closest_to(~"election"+"biden",30)
foo<-model %>% closest_to(~"politique" + "gouvernement",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la politique en temps de confinement")
foo<-model %>% closest_to(~"confin"+"appartement" - "maison" + "studio",40)
library(wordVectors)
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"Biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"biden",20)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
foo<-model %>% closest_to(~"biden",20)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
df<-tweets_11_06_2020 <- read_csv("~/AtelierR/Trump/tweets_11-06-2020.csv")
# Attention ajuster à sa configuration de dossier
df$date2<-as.POSIXlt(df$date)
readRDS("df_nrcliwc.rds")
foo<-readRDS("df_nrcliwc.rds")
fit<-lm(favorites~anger+anticipation+trust, data =foo)
summary(fit)
fit<-lm(favorites~anger+anticipation+trust+joy+fear+surprise, data =foo)
summary(fit)
fit<-lm(favorites~anger+anticipation+trust+joy+fear+surprise+ autority.virtue, data =foo)
fit<-lm(favorites~anger+anticipation+trust+joy+fear+surprise+ authority.virtue, data =foo)
summary(fit)
fit<-lm(favorites~anger+anticipation+trust+joy+fear+surprise+ authority.virtue+care.virtue, data =foo)
summary(fit)
fit<-lm(log(favorites)~anger+anticipation+trust+joy+fear+surprise+ authority.virtue+care.virtue, data =foo)
fit<-lm(log(favorites+1)~anger+anticipation+trust+joy+fear+surprise+ authority.virtue+care.virtue, data =foo)
summary(fit)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
# the devtools package needs to be installed for this to work
#devtools::install_github("kbenoit/quanteda.dictionaries",force=TRUE)
library(quanteda.dictionaries)
test<-liwcalike(df$text,  dictionary=data_dictionary_MFD)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
df<-tweets_11_06_2020 <- read_csv("~/AtelierR/Trump/tweets_11-06-2020.csv")
# Attention ajuster à sa configuration de dossier
df$date2<-as.POSIXlt(df$date)
writeRDS("df.drs")
write_rds("df.drs")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
df<-tweets_11_06_2020 <- read_csv("~/AtelierR/Trump/tweets_11-06-2020.csv")
# Attention ajuster à sa configuration de dossier
df$date2<-as.POSIXlt(df$date)
write_rds("df.drs")
df<-tweets_11_06_2020 <- read_csv("~/AtelierR/Trump/tweets_11-06-2020.csv")
# Attention ajuster à sa configuration de dossier
df$date2<-as.POSIXlt(df$date)
write_rds("df.drs")
write_rds(df,"df.drs")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
df<-tweets_11_06_2020 <- read_csv("~/AtelierR/Trump/tweets_11-06-2020.csv")
# Attention ajuster à sa configuration de dossier
df$date2<-as.POSIXlt(df$date)
write_rds(df,"df.drs")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
readRDS(file = "df.rds")
df<-readRDS(file = "df.rds")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
df<-readRDS(file = "df.rds")
df<-readRDS(file = "df.rds")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
1+1
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
read_rds("df.rds")
