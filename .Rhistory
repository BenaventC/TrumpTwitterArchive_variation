g10+scale_color_manual(values=col)
g10<-ggplot(data = df_emo, aes(x = date2, y = value, group = variable)) +
geom_line(aes(color=variable), size =0.8)+
theme_minimal()+
# geom_smooth(method = "gam",aes(color=variable))+
labs(title ="Evolution des émotions", x=NULL, subtitle = "lissage: 60 jours",y="valeur")+
geom_vline(xintercept = as.POSIXct("2016-11-04",format="%Y-%m-%d"), linetype="solid",color = "grey40", alpha=.5,size=3)
g10+scale_color_manual(values=col)
g10
df_emo<-melt(df_emo,id="date2")
col=c("orange3","orange1","chartreuse3","chartreuse1","skyblue3","skyblue2","grey50","grey90","purple3", "purple1")
g10<-ggplot(data = df_emo, aes(x = date2, y = value, group = variable)) +
geom_line(aes(color=variable), size =0.8)+
theme_minimal()+
# geom_smooth(method = "gam",aes(color=variable))+
labs(title ="Evolution des émotions", x=NULL, subtitle = "lissage: 60 jours",y="valeur")+
geom_vline(xintercept = as.POSIXct("2016-11-04",format="%Y-%m-%d"), linetype="solid",color = "grey40", alpha=.5,size=3)
g10
df$date2<-as.factor(df$date2)
df_emo<-melt(df_emo,id="date2")
col=c("orange3","orange1","chartreuse3","chartreuse1","skyblue3","skyblue2","grey50","grey90","purple3", "purple1")
g10<-ggplot(data = df_emo, aes(x = date2, y = value, group = variable)) +
geom_line(aes(color=variable), size =0.8)+
theme_minimal()+
# geom_smooth(method = "gam",aes(color=variable))+
labs(title ="Evolution des émotions", x=NULL, subtitle = "lissage: 60 jours",y="valeur")+
geom_vline(xintercept = as.POSIXct("2016-11-04",format="%Y-%m-%d"), linetype="solid",color = "grey40", alpha=.5,size=3)
g10
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
df<-tweets_11_06_2020 <- read_csv("~/AtelierR/Trump/tweets_11-06-2020.csv")
# Attention ajuster à sa configuration de dossier
df$date2<-as.POSIXlt(df$date)
## plot time series of tweets
ts_plot(df, "1 day", color="darkblue") +
theme_minimal() +
theme(plot.title = element_text(face = "bold")) + labs(
x = "nombre de tweets", y = "Nobre de tweets",
title = "Fréquence des posts twitters Donald Trump",
subtitle = "Nombre de tweets par heure"
)+  scale_x_datetime(date_breaks = "1 day", labels = scales::label_date_short())
df %>%
dplyr::group_by(isRetweet,isDeleted) %>%
ts_plot( "1 week") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold"),axis.text.x = element_text(size = 8, angle = 45)) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Fréquence des posts twitter de Donald Trump",
subtitle = "Nombre de tweets par  heures")+
scale_x_datetime(date_breaks = "1 month", labels = scales::label_date_short())
max<-max(df$favorites)
g02a<-ggplot(df,aes(x=favorites))+geom_histogram(fill="pink",alpha=0.8)+theme_minimal()+labs(title=paste0("distribution des favoris - max=",max))+scale_y_log10()
max<-max(df$retweets)
g02b<-ggplot(df,aes(x=retweets))+geom_histogram(fill="pink",alpha=0.5)+theme_minimal()+labs(title=paste0("distribution des retweets - max=",max))+scale_y_log10()
grid.arrange(g02a, g02b, ncol=1)
library(lubridate)
df<-df%>%mutate(year = year(date))
foo<-df%>% group_by(year) %>% summarise(retweets=sum(retweets),favorites=sum(favorites))
foo<-melt(foo, id="year")
ggplot(foo,aes(x=year, y=value, group=variable))+
geom_line(aes(color=variable), size=1.2)+
theme_minimal()+  scale_y_continuous(labels=comma)+labs(title=paste0("Evolution du cumul annuel des rt et likes des tweets de trump"))
df<-readRDS("df_nrcliwc.rds")
df$day<-as.numeric(format(df$date, "%d")) # jour
df$month<-as.numeric(format(df$date, "%m")) # mois
df$hour<-as.numeric(format(df$date, "%H")) # heure
df$Year<-as.numeric(format(df$date, "%Y")) # annnée
df$date2<-paste0(df$Year,"-",df$month,"-",df$day)
df$date2 <- as.POSIXct(strptime(df$date2, "%Y-%m-%d"))
df<-df%>% mutate(n_word=lengths(strsplit(text, "\\W+")) ,
nrc_positif=positive/n_word,
nrc_negatif =negative/n_word,
nrc_neutre=1-((positive+negative)/n_word),
nrc_valence=nrc_positif-nrc_negatif,
nrc_expressivity=nrc_positif+nrc_negatif,
)
g1<-ggplot(df,aes(x=nrc_positif))+geom_histogram()+theme_minimal() +labs(title="distribution des termes positifs")
g2<-ggplot(df,aes(x=nrc_negatif))+geom_histogram()+theme_minimal()+labs(title="distribution des termes négatifs")
g3<-ggplot(df,aes(x=nrc_neutre))+geom_histogram()+theme_minimal()+labs(title="distribution des termes neutres")
library(gridExtra)
grid.arrange(g1,g2,g3,ncol=1)
#library(ggtern)
#set.seed(1)
library(RcppRoll)
df_sent<-df %>%group_by(date2)%>%
summarise(sentiment=mean(nrc_valence, na.rm=TRUE),sentiment_exp=mean(nrc_expressivity, na.rm=TRUE))%>%
mutate(Sentiment=roll_mean(as.numeric(sentiment),60,na.rm = TRUE,fill=NA),Expressivite=roll_mean(as.numeric(sentiment_exp),60,na.rm = TRUE, fill=NA)) %>%
select(date2, Sentiment, Expressivite)
library(reshape2)
df_sent<-melt(df_sent,id="date2")
g10<-ggplot(data = df_sent, aes(x = date2, y = value, group = 1)) +
geom_line(aes(color=variable), size =0.8)+
theme_minimal()+
geom_smooth(method = "gam",aes(color=variable))+
labs(title ="Evolution du sentiment", x=NULL, subtitle = "lissage: 7 jours",y="valeur")+
geom_vline(xintercept = as.POSIXct("2016-11-04",format="%Y-%m-%d"), linetype="solid",color = "grey40", alpha=.5,size=3)+facet_wrap(vars(variable), ncol=1, scale="free")+scale_color_brewer(palette = "set2")
g10
#ggsave("evolutionmasque1.jpg",plot=last_plot(),width = 9, height = 6)
g10<-ggplot(data = df_sent, aes(x = date2, y = value, group = 1)) +
geom_line(aes(color=variable), size =0.8)+
theme_minimal()+
geom_smooth(method = "gam",aes(color=variable))+
labs(title ="Evolution du sentiment", x=NULL, subtitle = "lissage: 7 jours",y="valeur")+
geom_vline(xintercept = as.POSIXct("2016-11-04",format="%Y-%m-%d"), linetype="solid",color = "grey40", alpha=.5,size=3)+facet_wrap(vars(variable), ncol=1, scale="free")+scale_color_brewer(palette = "Set2")
g10
library(RcppRoll)
j=60
df_emo<-df %>%group_by(date2)%>%
summarise(anger=mean(anger, na.rm=TRUE),
anticipation=mean(anticipation, na.rm=TRUE),
disgust=mean(disgust, na.rm=TRUE),
fear= mean(fear, na.rm=TRUE),
joy= mean(joy, na.rm=TRUE),
sadness= mean(sadness, na.rm=TRUE),
surprise= mean(surprise, na.rm=TRUE),
trust= mean(trust, na.rm=TRUE)) %>%
select(date2, anger, anticipation, disgust, fear,joy,sadness,surprise,trust) %>%
mutate(anger=roll_mean(as.numeric(anger),j,na.rm = FALSE,fill=NA),
anticipation=roll_mean(as.numeric(anticipation),j,na.rm = TRUE,fill=NA),
disgust=roll_mean(as.numeric(disgust),j,na.rm = TRUE,fill=NA),
fear=roll_mean(as.numeric(fear),j,na.rm = TRUE,fill=NA),
joy=roll_mean(as.numeric(joy),j,na.rm = TRUE,fill=NA),
sadness=roll_mean(as.numeric(sadness),j,na.rm = TRUE,fill=NA),
surprise=roll_mean(as.numeric(surprise),j,na.rm = TRUE,fill=NA),
trust=roll_mean(as.numeric(trust),j,na.rm = TRUE,fill=NA))
df_emo<-melt(df_emo,id="date2")
g10<-ggplot(data = df_emo, aes(x = date2, y = value, group = variable)) +
geom_line(aes(color=variable), size =0.8)+
theme_minimal()+
# geom_smooth(method = "gam",aes(color=variable))+
labs(title ="Evolution des émotions", x=NULL, subtitle = "lissage: 60 jours",y="valeur")+
geom_vline(xintercept = as.POSIXct("2016-11-04",format="%Y-%m-%d"), linetype="solid",color = "grey40", alpha=.5,size=3)
g10
#ggsave("evolutionmasque1.jpg",plot=last_plot(),width = 9, height = 6)
library(RcppRoll)
j=60
df_emo<-df %>%group_by(date2)%>%
summarise(care.virtue=mean(care.virtue, na.rm=TRUE),
care.vice=-mean(care.vice, na.rm=TRUE),
fairness.virtue=mean(fairness.virtue, na.rm=TRUE),
fairness.vice= -mean(fairness.vice, na.rm=TRUE),
loyalty.virtue= mean(loyalty.virtue, na.rm=TRUE),
loyalty.vice = -mean(loyalty.vice, na.rm=TRUE),
authority.virtue= mean(authority.virtue, na.rm=TRUE),
authority.vice= -mean(authority.vice, na.rm=TRUE),
sanctity.virtue= mean(sanctity.virtue, na.rm=TRUE),
sanctity.vice= -mean(sanctity.vice, na.rm=TRUE)
) %>%
select(date2, care.virtue,care.vice,
fairness.virtue,fairness.vice,
loyalty.virtue,loyalty.vice,
authority.virtue,authority.vice,
sanctity.virtue,sanctity.vice
) %>%
mutate(care.virtue=roll_mean(as.numeric(care.virtue),j,na.rm = FALSE,fill=NA),
care.vice=roll_mean(as.numeric(care.vice),j,na.rm = TRUE,fill=NA),
fairness.virtue=roll_mean(as.numeric(fairness.virtue),j,na.rm = TRUE,fill=NA),
fairness.vice=roll_mean(as.numeric(fairness.vice),j,na.rm = TRUE,fill=NA),
loyalty.virtue=roll_mean(as.numeric(loyalty.virtue),j,na.rm = TRUE,fill=NA),
loyalty.vice=roll_mean(as.numeric(loyalty.vice),j,na.rm = TRUE,fill=NA),
authority.virtue=roll_mean(as.numeric(authority.virtue),j,na.rm = TRUE,fill=NA),
authority.vice=roll_mean(as.numeric(authority.vice),j,na.rm = TRUE,fill=NA),
sanctity.virtue=roll_mean(as.numeric(sanctity.virtue),j,na.rm = TRUE,fill=NA),
sanctity.vice=roll_mean(as.numeric(sanctity.vice),j,na.rm = TRUE,fill=NA)
)
df$date2<-as.factor(df$date2)
df_emo<-melt(df_emo,id="date2")
col=c("orange3","orange1","chartreuse3","chartreuse1","skyblue3","skyblue2","grey50","grey90","purple3", "purple1")
g10<-ggplot(data = df_emo, aes(x = date2, y = value, group = variable)) +
geom_line(aes(color=variable), size =0.8)+
theme_minimal()+
# geom_smooth(method = "gam",aes(color=variable))+
labs(title ="Evolution des émotions", x=NULL, subtitle = "lissage: 60 jours",y="valeur")
+
geom_vline(xintercept = as.POSIXct("2016-11-04",format="%Y-%m-%d"), linetype="solid",color = "grey40", alpha=.5,size=3)
g10<-ggplot(data = df_emo, aes(x = date2, y = value, group = variable)) +
geom_line(aes(color=variable), size =0.8)+
theme_minimal()+
# geom_smooth(method = "gam",aes(color=variable))+
labs(title ="Evolution des émotions", x=NULL, subtitle = "lissage: 60 jours",y="valeur")+
geom_vline(xintercept = as.POSIXct("2016-11-04",format="%Y-%m-%d"), linetype="solid",color = "grey40", alpha=.5,size=3)
g10
+scale_color_manual(values=col)
g10+scale_color_manual(values=col)
library(quanteda)
toks<-tokens(df$text)
col <-toks %>%
tokens_remove(stopwords("en")) %>%
textstat_collocations(min_count = 10,size=2:4, tolower = FALSE)%>% filter(abs(z)>5)
head(col, 20)
toks_comp <- tokens_compound(toks, pattern = col)
library(quanteda)
toks<-tokens(df$text)
col <-toks %>%
tokens_remove(stopwords("en")) %>%
tokens_select(pattern = "^[A-Z]", valuetype = "regex",
case_insensitive = FALSE, padding = TRUE) %>%
textstat_collocations(min_count = 10,size=2:4, tolower = FALSE)%>% filter(abs(z)>5)
head(col, 40)
toks_comp <- tokens_compound(toks_comp, pattern = col)
dfmat_tweets <- toks_comp %>%
dfm(remove_punct = TRUE, remove_url = TRUE, remove_symbols = TRUE) %>%
dfm_remove(pattern = c("*.tt", "*.uk", "*.com", "rt", "#*", "@*")) %>%
dfm_remove(pattern = stopwords("en"))
#ndoc(dfmat_tweets)
#topfeatures(dfmat_tweets)
dfmat_tweets %>%
textstat_frequency(n = 80) %>%
ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point(color="firebrick") +
coord_flip() +
labs(x = NULL, y = "Frequency") +
theme_minimal()
textplot_wordcloud(dfmat_tweets,min_count=200)
textplot_wordcloud(dfmat_tweets,min_count=300)
#l annee de la campgne
df$Year<-as.numeric(format(df$date, "%Y")) # annnée
foo<-df %>%select(-date2) %>% filter(Year>2019)
toks<-tokens(foo$text)
col <-toks %>%
tokens_remove(stopwords("en")) %>%
textstat_collocations(min_count = 10,size=2:4, tolower = FALSE)%>% filter(abs(z)>5)
head(col, 20)
toks_comp <- tokens_compound(toks, pattern = col)
dfmat_tweets <- toks_comp %>%
dfm(remove_punct = TRUE, remove_url = TRUE, remove_symbols = TRUE) %>%
dfm_remove(pattern = c("*.tt", "*.uk", "*.com", "rt", "#*", "@*","amp")) %>%
dfm_remove(pattern = stopwords("en"))
textplot_wordcloud(dfmat_tweets,min_count=100, color = rev(RColorBrewer::brewer.pal(10, "RdBu")))
textplot_wordcloud(dfmat_tweets,min_count=300, color = rev(RColorBrewer::brewer.pal(10, "RdBu")))
textplot_wordcloud(dfmat_tweets,min_count=30, color = rev(RColorBrewer::brewer.pal(10, "RdBu")))
textplot_wordcloud(dfmat_tweets,min_count=50, color = rev(RColorBrewer::brewer.pal(10, "RdBu")))
tag_fcm <- fcm(dfmat_tweets)
toptag <- names(topfeatures(tag_fcm, 500))
head(tag_fcm)
topgat_fcm <- fcm_select(tag_fcm, pattern = toptag)
textplot_network(topgat_fcm, min_freq = 30,color="pink", edge_alpha = 0.2, edge_size = 2,vertex_size=.7, vertex_labelsize = 3.5)
library("quanteda.textmodels")
#mylsa <- textmodel_lsa(dfmat_tweets)
#si le fichier on lit nos données brutes, et on nettoie le corpus des stop words
cnlp_init_udpipe(model_name  = "french")
#lecture de l'ensemble de nos tweets
obj<-df$text
#Annotation des tweets afin de pouvoir identifier les stopwords
Vocab<-cnlp_annotate(obj)
#Annotation des tweets afin de pouvoir identifier les stopwords
Vocab<-cnlp_annotate(obj,verbose=10000)
#filtrage sur les stopwords
foo<-as.data.frame(Vocab[c("token")])
ggplot(foo,aes(x=token.relation))+
geom_bar()+
coord_flip()
#on filtte advebes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.upos %in% c('ADV','ADJ','VERB', 'NOUN'))
#on crée une chaine de caractère qui concatène les lemmes filtrés
all_tweets <- paste(updated_vocab['token.lemma'], sep= " ")
#on génère le fichier de ces twitts " purifiés"
write.table(all_tweets, file="tweets.txt")
#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
prep_word2vec(origin="tweets.txt",destination="confinement_vec.txt",lowercase=T,bundle_ngrams=4, Verbose=FALSE)
#library(cleanNLP) pour les POS et Dépendences syntaxiques
#library(wordvectors)
# initialisation du modèle , ici udpipe, mais aussi spacy corenlp ou stringi
#(un travail devrait être de comprer ces méthodes par le taux de couvrement!!!!)
cnlp_init_udpipe(model_name  = "french")
#library(cleanNLP) pour les POS et Dépendences syntaxiques
library(wordvectors)
remotes::install_github("bmschmidt/wordVectors")
#library(cleanNLP) pour les POS et Dépendences syntaxiques
install.packages("remotes")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
df<-tweets_11_06_2020 <- read_csv("~/AtelierR/Trump/tweets_11-06-2020.csv")
# Attention ajuster à sa configuration de dossier
df$date2<-as.POSIXlt(df$date)
remotes::install_github("bmschmidt/wordVectors")
#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
prep_word2vec(origin="tweets.txt",destination="confinement_vec.txt",lowercase=T,bundle_ngrams=4, Verbose=FALSE)
#library(cleanNLP) pour les POS et Dépendences syntaxiques
install.packages("remotes")
install.packages("remotes")
remotes::install_github("bmschmidt/wordVectors")
#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
prep_word2vec(origin="tweets.txt",destination="confinement_vec.txt",lowercase=T,bundle_ngrams=4, Verbose=FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr)
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
df<-tweets_11_06_2020 <- read_csv("~/AtelierR/Trump/tweets_11-06-2020.csv")
# Attention ajuster à sa configuration de dossier
df$date2<-as.POSIXlt(df$date)
model = train_word2vec("trump_vec.txt","trump.bin",vectors=250
,threads=4,window=5,iter=10,negative_samples=0,force=TRUE, min_count=50)
library(wordVectors)
model = train_word2vec("trump_vec.txt","trump.bin",vectors=250
,threads=4,window=5,iter=10,negative_samples=0,force=TRUE, min_count=50)
foo<-model %>% closest_to(~"mask",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"Biden",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"biden",40)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
View(foo)
View(foo)
foo<-model %>% closest_to(~"biden",10)
foo = foo [-1:-3,]
foo<-model %>% closest_to(~"trump",10)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"great",10)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"President",10)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
View(Vocab)
View(Vocab)
#on filtte adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.lemma %in% c('ADV','ADJ','VERB', 'NOUN'))
#filtrage sur les stopwords
foo<-as.data.frame(Vocab[c("token")])
#on filtte adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.lemma %in% c('ADV','ADJ','VERB', 'NOUN'))
#on crée une chaine de caractère qui concatène les lemmes filtrés
all_tweets <- paste(updated_vocab['token.lemma'], sep= " ")
# initialisation du modèle , ici udpipe, mais aussi spacy corenlp ou stringi
#(un travail devrait être de comprer ces méthodes par le taux de couvrement!!!!)
cnlp_init_udpipe(model_name  = "english")
library(cleanNLP) pour les POS et Dépendences syntaxiques
library(cleanNLP) #pour les POS et Dépendences syntaxiques
# initialisation du modèle , ici udpipe, mais aussi spacy corenlp ou stringi
#(un travail devrait être de comprer ces méthodes par le taux de couvrement!!!!)
cnlp_init_udpipe(model_name  = "english")
#lecture de l'ensemble de nos tweets
obj<-df$text
#Annotation des tweets afin de pouvoir identifier les stopwords
t0<-Sys.time() #date de départ
Vocab<-cnlp_annotate(obj,verbose=5000)
t1<-Sys.time() #date de fin.... juste pour controler une opération qui peut prendre 40 mn sur un processeeur 4 couer à 3.6ghz et 32g de ram.
#filtrage sur les stopwords
foo<-as.data.frame(Vocab[c("token")])
ggplot(foo,aes(x=token.upos))+
geom_bar()+coord_flip() +
theme_minimal()
ggplot(foo,aes(x=token.relation))+
geom_bar()+
coord_flip()
#on filtte adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.lemma %in% c('ADV','ADJ','VERB', 'NOUN'))
#on crée une chaine de caractère qui concatène les lemmes filtrés
all_tweets <- paste(updated_vocab['token.lemma'], sep= " ")
#on génère le fichier de ces twitts " purifiés"
write.table(all_tweets, file="tweets.txt")
#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
prep_word2vec(origin="tweets.txt",destination="trump_vec.txt",lowercase=T,bundle_ngrams=4)
model = train_word2vec("trump_vec.txt","trump.bin",vectors=250
,threads=4,window=5,iter=10,negative_samples=0,force=TRUE, min_count=50)
#filtrage sur les stopwords
foo<-as.data.frame(Vocab[c("token")])
#on filtte adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.lemma %in% c('ADV','ADJ','VERB', 'NOUN'))
#on filtte adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.token %in% c('ADV','ADJ','VERB', 'NOUN'))
#filtrage sur les stopwords
foo<-as.data.frame(Vocab[c("token")])
ggplot(foo,aes(x=token.upos))+
geom_bar()+coord_flip() +
theme_minimal()
ggplot(foo,aes(x=token.relation))+
geom_bar()+
coord_flip()
#on filttre adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.upos%in% c('ADV','ADJ','VERB', 'NOUN'))
#on crée une chaine de caractère qui concatène les lemmes filtrés
all_tweets <- paste(updated_vocab['token.lemma'], sep= " ")
#on génère le fichier de ces twitts " purifiés"
write.table(all_tweets, file="tweets.txt")
#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
prep_word2vec(origin="tweets.txt",destination="trump_vec.txt",lowercase=T,bundle_ngrams=4)
model = train_word2vec("trump_vec.txt","trump.bin",vectors=250
,threads=4,window=5,iter=10,negative_samples=0,force=TRUE, min_count=50)
foo<-model %>% closest_to(~"President",10)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"president",10)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"biden",10)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"joebiden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~ "mask",60)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la solidarite")
g1
foo<-model %>% closest_to(~ "election",60)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la solidarite")
g1
foo<-model %>% closest_to(~ "election",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la solidarite")
g1
foo<-model %>% closest_to(~"election"+"biden",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la protection")
g1
foo<-model %>% closest_to(~"election"+"biden",30)
foo<-model %>% closest_to(~"politique" + "gouvernement",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la politique en temps de confinement")
foo<-model %>% closest_to(~"confin"+"appartement" - "maison" + "studio",40)
library(wordVectors)
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"Biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
foo<-model %>% closest_to(~"biden",15)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
foo<-model %>% closest_to(~"biden",20)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
foo<-model %>% closest_to(~"biden",20)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word, Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de la dimension sanitaire")
g1
