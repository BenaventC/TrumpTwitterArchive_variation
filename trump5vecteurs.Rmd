---
title: "TrumpArchive 5 - Vecteur trump"
author: "MirM"
date: "07/11/2020"
output: html_document
bibliography : MIrM.bib
---

Il s'agit dans cette analyse d'utiliser les resources du modèle word2vec de Mikolov(2013) pour caractériser ce que trump dit de biden, mais aussi de lui même.

## la méthode Word2vec

Pour l'application on utilise le package [`wordVectors`](https://github.com/bmschmidt/wordVectors)

## Libraries et données


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) #la boite à outil universelle
library(syuzhet) #analyse du sentiment
library(readr) #pour lire du csv
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate) #c'est pour traiter les format time
library(quanteda) #en avant le nlp
library(cleanNLP) # un super annotateur

#les datas sont là

df<-read_rds("df.rds")

```


## Préparer et annoter grammaticalement les données


En lecture word2vec


On prépare les données en " résumant" les tweets à leur plus simple expression

d'abord on tokenize, et on réduit les tokens ( les token) en suprrimant les symbole, les nombre, en mettant en minuscules etc 

```{r prep1, warning=FALSE, message=FALSE, eval=TRUE}
#lecture de l'ensemble de nos tweets
obj<-df$text 
foo<-tokens(obj, remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_url = TRUE,
  remove_separators = TRUE,
  split_hyphens = FALSE,
  padding = FALSE) %>%
  tokens_remove(pattern = c("*.tt", "*.uk", "*.com", "rt", "#*", "@*","amp", "RT")) %>%
  tokens_select(pattern="<U+.*",  selection = "remove", valuetype = "regex")%>%
  tokens_tolower() 

foo1<-data.frame(
  id = seq_along(foo),
  text = sapply(foo, paste, collapse = " "),
  row.names = NULL
)
```

et on fait de l'annotations POS, ainsi que des dépendences syntaxiques


```{r prep2, warning=FALSE, message=FALSE, eval=FALSE}
library(cleanNLP) #pour les POS et Dépendences syntaxiques
# initialisation du modèle , ici udpipe, mais aussi spacy corenlp ou stringi
#(un travail devrait être de comprer ces méthodes par le taux de couvrement!!!!)
cnlp_init_udpipe(model_name  = "english")


#library(cleanNLP)
#Annotation des tweets afin de pouvoir identifier les stopwords
t0<-Sys.time() #date de départ
Vocab<-cnlp_annotate(foo1$text,verbose=5000)
t1<-Sys.time() #date de fin.... juste pour controler une opération qui peut prendre 40 mn sur un processeeur 4 coeurs à 3.6ghz et 32g de ram.
t<-t1-t0
t
write_rds(Vocab,"Vocab.rds")
```

Et un peu de filtrage ...

```{r prep3, warning=FALSE, message=FALSE, eval=TRUE}
Vocab <-read_rds("Vocab.rds")

foo<-as.data.frame(Vocab[c("token")])

ggplot(foo,aes(x=token.upos))+
  geom_bar(fill="Gold3")+
  coord_flip() +
  theme_minimal()
ggplot(foo,aes(x=token.relation))+
  geom_bar(fill="Gold2")+
  coord_flip() +theme_minimal()

#on filtre adverbes adjectifs verb et non communs
updated_vocab <- foo %>% filter(token.upos %in% c('ADV','ADJ','VERB', 'NOUN'))
#on crée une chaine de caractère qui concatène les lemmes filtrés
all_tweets <- paste(updated_vocab['token.lemma'], sep= " ")
#on génère le fichier de ces tweets "purifiés"
write.table(all_tweets, file="tweets.txt")
```

# préprocessing spécifique

```{r prep4, warning=FALSE, message=FALSE, eval=TRUE}
#install.packages("remotes")
#remotes::install_github("bmschmidt/wordVectors")
library(wordVectors)

#Nettoyage des tweets et identification des n-grammes en vue d'entraîner le modèle
prep_word2vec(origin="tweets.txt",destination="trump_vec.txt",lowercase=T,bundle_ngrams=4)

```

## Faire tourner le modèle

en variant vecteurs et fenêtre.

```{r train, warning=TRUE, message=TRUE}
#Création et entraînement du modèle vectoriel

model = train_word2vec("trump_vec.txt","trump.bin",vectors=200,threads=3,window=5,iter=10,negative_samples=0,force=TRUE, min_count=20)


```

## Exploiter le modèle

###

```{r}
foo<-model %>% closest_to(~"biden",30)
foo = foo [-1:-3,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word,Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+ggtitle("N-grammes proches de Biden")
g1
```

```{r solidarité}
foo<-model %>% closest_to(~ "trump",30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word,Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de Trump")
g1
```

```{r}
foo<-model %>% wordVectors::closest_to(~("trump"+"biden"),30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word,Similarity),y=Similarity))+geom_point(col="black",size=3)+coord_flip()+theme_minimal()+scale_y_log10()+ggtitle("N-grammes proches de la trump+biden")
g1
```


```{r}
foo<-model %>% wordVectors::closest_to(~("trump"-"biden"),30)
foo = foo [-1:-2,]
foo$Similarity<-foo[,2]
g1<-ggplot(foo, aes(x=reorder(word,Similarity),y=Similarity))+geom_point(col="black",size=3)+
  coord_flip()+
  theme_minimal()+
  scale_y_log10()+ggtitle("N-grammes proches de trump-Biden")
g1
```
## Avec tsne

```{r Télétravail, fig.height=9, fig.width=9}
q_words = c("trump", "biden")
term_set = lapply(q_words, 
                  function(q_word) {
                    nearest_words = model %>% closest_to(model[[q_word]],80)
                    nearest_words$word
                  }) %>% unlist
subset = model[[term_set,average=F]]
#subset1<-as.data.frame(subset@.Data)
#clus<-hclust(subset)

library(Rtsne)
library(RColorBrewer)
# run Rtsne with default parameters
set.seed(57)
rtsne_out <- Rtsne(as.matrix(subset), perplexity=10)
# plot the output of Rtsne
#jpeg("fig.jpg", width=2400, height=1800)
color.vec = c("#556270", "#4ECDC4", "#1B676B", "#FF6B6B", "#C44D58", "seagreen1", "seagreen4", "slateblue4")
#clus<-as.data.frame(clus)
#clus$word<-rownames(clus)
terms<-as.data.frame(rownames(subset))
terms$word<-terms[,1] 
#terms<-terms %>% left_join(clus, by = "word")
plot(rtsne_out$Y, t='n')
#count(terms, clus)$n[2]
text(rtsne_out$Y, labels=rownames(subset),cex=0.8)#col=color.vec[terms$clus])
```

