"0",""
"0","df_noun<-foo %>% select(id,doc_id,token,lemma)"
"0",""
"0","df_noun1<-na.omit(df_noun)"
"0",""
"0","#un fichier index pour associer les doc_id à un index texte"
"0","lem01<-df_noun %>% mutate(n=1)%>%"
"0","  group_by(doc_id) %>%"
"0","  summarize(n_id = sum(n))"
"0","lem01$i_t<-as.numeric(rownames(lem01))"
"0",""
"0","df_noun2<-lem01 %>% left_join(df_noun,by=""doc_id"") "
"0",""
"0","################### la boucle ####################################"
"0","#pour réécrire les tweets avec les token filtrés"
"0","#il doit y avoir une solution plus élégante avec lapply"
"0",""
"0","#on initialise les fichiers temporaire"
"0","foo1<-data.frame(matrix( nrow=1, ncol=3))"
"0","foo1$text<-""xxx"""
"0","foo1$i_t<-0"
"0","foo1<- foo1 %>% as.data.frame() %>% select(-X1,-X2,-X3)"
"0",""
"0","foo2<-data.frame(matrix( nrow=1, ncol=3))"
"0","foo2$text<-""xxx"""
"0","foo2$i_t<-0"
"0","foo2<- foo2 %>% as.data.frame() %>% select(-X1,-X2,-X3)"
"0",""
"0","i=1"
"0","t1=Sys.time()"
"0",""
"0","#la boucle permet de crer le texte mais aussi d'échantillonner au cours du temps avec un pas constant"
"0","for (i in seq(1,55090)) {"
"0","updated_vocabi<-df_noun2 %>% dplyr::filter(i_t==i) "
"0","foo2$text <- paste(updated_vocabi[""lemma""], sep= "" "") #on retient les lemmes"
"0","foo2$text<-substring(foo2$text, 3) #on elimine les 3 premier caractères"
"0","foo2$text<-gsub(""-"", """", foo2$text, fixed=TRUE) #on supprime les tirêt"
"0","foo2$text<-gsub(""[[:punct:]]"", """", foo2$text) # toute la ponctuation et les slash"
"0","foo2$text <- iconv(foo2$text, to=""ASCII//TRANSLIT//IGNORE"")"
"0","foo2$text<-gsub(""NA"", """", foo2$text)"
"0","foo2$i_t<-i"
"0","foo1<-rbind(foo1,foo2)"
"0","}"
"0","foo1<-foo1 %>% filter(i_t>0)"
"0","saveRDS(foo1, ""vocabulaire.rds"")"
"0","t2=Sys.time()"
"0","time=t2-t1"
"0","time"
"1","Time difference of "
"1",""
"1","4.542529"
"1",""
"1"," "
"1",""
"1","mins"
"1",""
"1","
"
