"0",""
"0","#nettoyage des donn√©es"
"0","##les liens"
"0",""
"0","data_clean <- data %>%"
"0","  mutate(top = retweets_cl,"
"0","         text = str_replace_all(text, "" ?(f|ht)tp(s?)://(.*)[.][a-z]+"", """")) %>%"
"0","  select(id, top, text)"
"0",""
"0",""
"0","## les stop_word"
"0",""
"0","data_counts <- map_df(1:3,"
"0","                      ~ unnest_tokens(data_clean, word, text, "
"0","                                      token = ""ngrams"", n = .x)) %>%"
"0","  anti_join(stop_words, by = ""word"") %>%"
"0","  count(id, word, sort = TRUE)"
"0",""
"0",""
"0","## mots assez frequents"
"0","words_10 <- data_counts %>%"
"0","  group_by(word) %>%"
"0","  summarise(n = n()) %>% "
"0","  filter(n >= 30) %>%"
"0","  select(word)%>%drop_na()"
"0",""
"0","#we will right-join this to our data.frame before we will calculate the tf_idf and cast it to a document term matrix."
"0",""
"0","data_dtm <- data_counts %>%"
"0","  right_join(words_10, by = ""word"") %>%"
"0","  bind_tf_idf(word, id, n) %>%"
"0","  cast_dtm(id, word, tf_idf)"
"0",""
"0",""
"0","#We create this meta data.frame which acts as a intermediate from our first data set since some tweets might have disappeared completely after the reduction."
"0",""
"0","meta <- tibble(id = as.character(dimnames(data_dtm)[[1]])) %>%"
"0","  left_join(data_clean[!duplicated(data_clean$id), ], by = ""id"")"
"0",""
