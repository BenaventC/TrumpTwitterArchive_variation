---
title: "trump"
author: "cb"
date: "07/11/2020"
output: html_document
bibliography : MIrM.bib
---

# Data 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rtweet)
library(syuzhet)
library(readr) 
library(scales) # pour affiner les échelles de ggplot
library(gridExtra) #pour associer des graph en une figure
library(reshape2) #pour faire des fichiers " longs"
library(lubridate)
library(quanteda)
library(tidyr)

df<-readRDS("df_nrcliwc.rds")
```

## Introduction

L'analyse de topic est désormais bien répandue depuis le premier modèle crée par @blei_latent_2003
L'analyse de topic est devenu un classique tu traitement du langage naturel. proposé par  blei en 2003, elle a connu différente variante dont une nous serait utile mais la question du temps

il faut lemmatiser

du lda avec stm

c'est un modèle particulier de topic qu permet d'introduire des co-variables. La nôtre est le temps appréhendé à la semaine
Roberts, Margaret E., Brandon M. Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson, and David G. Rand. "Structural Topic Models for Open-Ended Survey Responses." American Journal of Political Science 58, no 4 (2014): 1064-1082.


## Construction du modèle


### préparation des données

On veut filtrer sur les noms communs

On peut penser à d'autres filtrages ( uniquement les tweets de trump par exemple)

```{r stm01}
#chargement du fichier des textes et les termes annotés
# le fichier annoté pour l'analyse lexicale
obj<-readRDS("Vocab.rds")
#le fichier d'origine augmenté des sentiment
df<-readRDS(file = "df_nrcliwc.rds") %>%select(-date2)
df$doc_id<-as.numeric(rownames(df))

#calcul du nombre de termes
token<-obj$token %>%
  group_by(doc_id) %>%
  summarize(length = n())
#ggplot(token,aes(length))+geom_histogram()

#ajout au fichier des textes
df<-df%>%left_join(token, by="doc_id")
#determination de l'année
df$year<-as.numeric(format(df$date, "%Y")) # mois

#ajout des info textes pour chacun des termes et filtrage sur les noms communs et les textes ayant au moins deux mots
foo<-obj$token %>% left_join(df, by="doc_id")
foo<-foo %>% filter( upos=="NOUN" & length>2)

```

## On va réécrire les tweets mais après les avoir condensé

```{r stm02}

df_noun<-foo %>% select(id,doc_id,token,lemma)

df_noun1<-na.omit(df_noun)

#un fichier index pour associer les doc_id à un index texte
lem01<-df_noun %>% mutate(n=1)%>%
  group_by(doc_id) %>%
  summarize(n_id = sum(n))
lem01$i_t<-as.numeric(rownames(lem01))

df_noun2<-lem01 %>% left_join(df_noun,by="doc_id") 

################### la boucle ####################################
#pour réécrire les tweets avec les token filtrés
#il doit y avoir une solution plus élégante avec lapply

#on initialise les fichiers temporaire
foo1<-data.frame(matrix( nrow=1, ncol=3))
foo1$text<-"xxx"
foo1$i_t<-0
foo1<- foo1 %>% as.data.frame() %>% select(-X1,-X2,-X3)

foo2<-data.frame(matrix( nrow=1, ncol=3))
foo2$text<-"xxx"
foo2$i_t<-0
foo2<- foo2 %>% as.data.frame() %>% select(-X1,-X2,-X3)

i=1
t1=Sys.time()

#la boucle permet de crer le texte mais aussi d'échantillonner au cours du temps avec un pas constant
for (i in seq(1,55090)) {
updated_vocabi<-df_noun2 %>% dplyr::filter(i_t==i) 
foo2$text <- paste(updated_vocabi["lemma"], sep= " ") #on retient les lemmes
foo2$text<-substring(foo2$text, 3) #on elimine les 3 premier caractères
foo2$text<-gsub("-", "", foo2$text, fixed=TRUE) #on supprime les tirêt
foo2$text<-gsub("[[:punct:]]", "", foo2$text) # toute la ponctuation et les slash
foo2$text <- iconv(foo2$text, to="ASCII//TRANSLIT//IGNORE")
foo2$text<-gsub("NA", "", foo2$text)
foo2$i_t<-i
foo1<-rbind(foo1,foo2)
}
foo1<-foo1 %>% filter(i_t>0)
saveRDS(foo1, "vocabulaire.rds")
t2=Sys.time()
time=t2-t1
time

```


## On prépare pour stm



```{r stm03}

##la préparation pour stm
text_filtered<-readRDS("vocabulaire.rds")
text_filtered<-text_filtered %>% left_join(lem01, by=c("i_t"))
df_user<-df%>%select(id,year,retweets,doc_id)%>%mutate(retweets=log10(retweets+1))



text_filtered<-text_filtered %>% left_join(df_user, by=c("doc_id")) 

text_filtered <- text_filtered%>% filter(year>2011)

ggplot(text_filtered, aes(x=year))+geom_bar()
#dfm_sample<-sample_n(text_filtered,40000)

corp<-corpus(text_filtered$text, docvars=(text_filtered))# corps des auteueut

set.seed(100)

#head(cols <- textstat_collocations(corp, size = 2, min_count = 2), 10)

dfm<-dfm(corp, tolower = TRUE,
         remove_punct = TRUE, 
         remove_numbers = FALSE,
         remove = stopwords("english"),
  stem = FALSE,  verbose = quanteda_options("verbose"))
#library("stm")


dfm_stm <- convert(dfm, to = "stm")
```

## Estimation du modèle


```{r stm04}
# le nombre de topics choisi
library(stm)
k=12
# la spécification du modèle
set.seed(2020)
model.stm <- stm(dfm_stm$documents, 
                 dfm_stm$vocab, 
                 K = k, 
                 max.em.its = 25,
                 data = dfm_stm$meta, 
                 init.type = "Spectral", 
                 prevalence =~ s(year)+s(retweets),
                 interactions = FALSE,
                 verbose = TRUE) # this is the actual stm call
label<-as.data.frame(labelTopics(model.stm, n = k)$score)
labelTopics(model.stm)

```

```{r stm05}

#les 4 scores

plot(model.stm, type = "summary", labeltype="prob",text.cex = 0.7,n=7)
plot(model.stm, type = "summary", labeltype="score",text.cex = 0.7,n=5)
plot(model.stm, type = "summary", labeltype="lift",text.cex = 0.7,n=5)
plot(model.stm, type = "summary", labeltype="frex",text.cex = 0.7,n=5)

#la qualité des topic

topicQuality(model.stm , dfm_stm$documents, xlab = "Semantic Coherence",  ylab = "Exclusivity", M = k)

```


## la description des topics


avec LDA vis
https://ldavis.cpsievert.me/reviews/reviews.html


```{r stmvis, fig.width=12}


library(LDAvis)
#ldavis<-toLDAvisJson(mod=model.stm, docs=dfm_stm$documents)
#serVis(ldavis, out.dir = 'vis', open.browser = TRUE)



```



type model ?

```{r stm07, fig.width=9}

par(mfrow = c(3,4) , mar = c(0,0,0,0))
for (i in seq_along((1:k)))
{
  cloud(model.stm, topic = i, type ="model", max.words = 20, colors="darkblue", random.order=FALSE)
  text(x=0.5, y=1, paste0("topic",i))

}
ggsave("cloud01.jpg",plot=last_plot(),width = 9, height = 6)
```

type model doc ?

```{r stm08, fig.width=12}

par(mfrow = c(3,4) , mar = c(0,0,0,0))
for (i in seq_along((1:k)))
{
cloud(model.stm, topic = i,type = c("model","documents"), dfm,thresh = 0.1, max.words = 20, colors="firebrick")
   text(x=0.5, y=1, paste0("topic",i))
}
ggsave("cloud02.jpg",plot=last_plot(),width = 9, height = 6)

```

### prevalence temps


```{r stm009, fig.width=12}

model.stm.labels <- labelTopics(model.stm, 1:k)

dfm_stm$meta$datum <- as.numeric(dfm_stm$meta$year)

model.stm.ee <- estimateEffect(1:k ~ s(year), model.stm, meta = dfm_stm$meta)

par(mfrow = c(3,4) , mar = c(1,0,2,0))
for (i in seq_along((1:k)))
{
  plot(model.stm.ee, "year", method = "continuous", topics = i, main = paste0(model.stm.labels$score[i,1:4], collapse = "-"), printlegend = T)

}
ggsave("prevalence.jpg",plot=last_plot(),width = 9, height = 6)
```

prevalence semaine . Chaque document estmodélisé comme un mélange de plusieurs sujets. La prévalence thématique indique dans quelle mesure chaque sujet contribue à un document. Comme les différents documents proviennent de différentes sources, il est naturel de vouloir laisser cette prévalence varier en fonction des métadonnées dont nous disposons sur les documents sources, en l'occurence ici c'est le temps avec pour unité la semaine.



### prevalence temps


```{r stm010, fig.width=12}

model.stm.labels <- labelTopics(model.stm, 1:k)

dfm_stm$meta$retweets <- as.numeric(dfm_stm$meta$retweets)

model.stm.ee <- estimateEffect(1:k ~ s(retweets), model.stm, meta = dfm_stm$meta)

par(mfrow = c(3,4) , mar = c(1,0,2,0))
for (i in seq_along((1:k)))
{
  plot(model.stm.ee, "retweets", method = "continuous", topics = i, main = paste0(model.stm.labels$score[i,1:4], collapse = "-"), printlegend = T)

}
ggsave("prevalence.jpg",plot=last_plot(),width = 9, height = 6)

```

retrouver les textes liés aux topic

et regarder les liens ( plutôt positif) entre les topics. L'absence de lien dénote l'existance possible d'une relation négative

semantic coherence is a metric related to pointwise mutual information that was introduced in a paper by David Mimno, Hanna Wallach and colleagues (see references), The paper details a series of manual evaluations which show that their metric is a reasonable surrogate for human judgment. The core idea here is that in models which are semantically coherent the words which are most probable under a topic should co-occur within the same document.

## la structure des topics


```{r stm06, fig.width=15, fig.width=12}

library(igraph)
b<-NULL
for (i in seq_along((1:k)))
{
  a<-paste0(model.stm.labels$score[i,1:3], collapse = "\n")
  a<-paste("Topic",i,"\n",a)
b<-rbind(b,a)
}

label<-as.data.frame(b)
label
topicor<-topicCorr(model.stm, method = "simple", cutoff=0.10,verbose = TRUE)

adjmatrix <-topicor[[2]]
theta <-model.stm[[7]]
thetat<-melt(theta)
thetat<-thetat %>%group_by(Var2)%>%summarise(mean=mean(value))
cbind(label,thetat)

g<-graph_from_adjacency_matrix(adjmatrix, mode = "lower", weighted = TRUE, diag = FALSE, add.colnames = FALSE, add.rownames = b)
g <- delete.edges(g, E(g)[ abs(weight) < 0.1])

curve_multiple(g)

set.seed(2021)
plot(g,layout=layout_with_fr,  margin = c(0, 0, 0, 0),
     edge.width=abs(E(g)$weight)*15,
     edge.color=ifelse(E(g)$weight > 0, "grey60","red"),
     vertex.label=label$V1,
     vertex.label.family="Arial",
     vertex.color = adjustcolor("pink2", alpha.f = .2),vertex.label.cex=0.7, vertex.size=400*thetat$mean, vertex.frame.color= "white"
     )
ggsave("topicnetwork1.jpg",plot=last_plot(),width = 12, height = 9)
```



## Références
